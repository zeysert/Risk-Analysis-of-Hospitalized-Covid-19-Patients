{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4b5335bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import Python's data analysis library\n",
    "import pandas as pd\n",
    "\n",
    "#We imported our CSV file data to be processed as data variable\n",
    "data = pd.read_csv(\"C:/Users/dekk/Desktop/ml/covid19_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e0e299f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We calculated patient height and weight means to missing places in CSV file\n",
    "height_mean = data['HEIGHT'].mean()\n",
    "weight_mean = data['WEIGHT'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "254a10eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We imputed numerical missing values with their means seperately\n",
    "data['HEIGHT'] = data['HEIGHT'].fillna(height_mean)\n",
    "data['WEIGHT'] = data['WEIGHT'].fillna(weight_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f8b80946",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We dropped null clinical values\n",
    "data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "60da60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We drop IDs either\n",
    "data.drop(['ID'], axis=1 ,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f152d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We represented YES/NO values as 1/0\n",
    "data[\"INTUBATION\"] = data[\"INTUBATION\"].map({'YES': 1, 'NO': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "668cd8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We started to classify INTUBATION data\n",
    "#Pandas drop columns using list of column names\n",
    "X = data.drop(['INTUBATION', 'INTENSIVE CARE'], axis=1)\n",
    "Y = data[\"INTUBATION\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "48a05abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1362, 34)\n",
      "(1362,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Printing out our model shapes\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "84d6ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#We imported scikit-learn's train_test_split library to split those each\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "72570af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We assigned values in train_test_split function, Test Group is assigned as %25 percent and the rest is defined as Training Group\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X ,Y, test_size = 0.25, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0d5df8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=0)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#We imported RandomForestClassifier and fitted our Train data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 200, random_state = 0)\n",
    "classifier.fit(X_Train,Y_Train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "312eedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Predicting Test data throughout classifier variable\n",
    "Y_Pred = classifier.predict(X_Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "33460729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9560117302052786\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Printing out the Random Forest accuracy score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "accuracy_score_rf = accuracy_score(Y_Test, Y_Pred)\n",
    "print(accuracy_score_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c24afc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST ALGORITHM ACCURACY SCORE\n",
      "0.96 %\n"
     ]
    }
   ],
   "source": [
    "print(\"RANDOM FOREST ALGORITHM ACCURACY SCORE\")\n",
    "print(round(accuracy_score_rf,2,), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e2cf4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Receiving algorithm performance measure formulas as below\n",
    "tn, fp, fn, tp = confusion_matrix(Y_Test, Y_Pred).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "065eb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We imported Python's Numpy library and created two empty arrays\n",
    "import numpy as np\n",
    "sensitivity_rf = np.empty(0)\n",
    "specificity_rf = np.empty(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0d86d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving Specificity and Sensitivity formulas from 4 variable and assigned those arrays\n",
    "sensitivity_rf = np.append(sensitivity_rf, np.array([tp / (tp + fn)]))\n",
    "specificity_rf = np.append(specificity_rf, np.array([tn / (tn + fp)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6f818998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "#Printing out the Specificity and Sensitivity variables\n",
    "print(sensitivity_rf)\n",
    "print(specificity_rf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f44819b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We imported GradientBoostingClassifier and fitted our Train data\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gradient_boosting = GradientBoostingClassifier(n_estimators = 100, max_depth = 3)\n",
    "gradient_boosting.fit(X_Train, Y_Train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b4e25016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.8 %\n"
     ]
    }
   ],
   "source": [
    "#Printing out the Gradient Boosting accuracy score\n",
    "acc_gradient = round(gradient_boosting.score(X_Train, Y_Train) * 100, 2)\n",
    "print(round(acc_gradient,2,), \"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "96f23a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9413489736070382\n"
     ]
    }
   ],
   "source": [
    "#Predicting Test data throughout classifier variable and process same operations as we did before\n",
    "prediction_gb = gradient_boosting.predict(X_Test)\n",
    "accuracy_score_gb = accuracy_score(prediction_gb, Y_Test)\n",
    "print(accuracy_score_gb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3a2e6d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25]\n",
      "[0.98442368]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_Test, prediction_gb).ravel()\n",
    "sensitivity_gb = np.empty(0)\n",
    "specificity_gb = np.empty(0)\n",
    "sensitivity_gb = np.append(sensitivity_gb, np.array([tp / (tp + fn)]))\n",
    "specificity_gb = np.append(specificity_gb, np.array([tn / (tn + fp)]))\n",
    "print(sensitivity_gb)\n",
    "print(specificity_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "35aed24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Now, we are about to classify INTENSIVE CARE data\n",
    "#Same operation proceeded like before\n",
    "data[\"INTENSIVE CARE\"] = data[\"INTENSIVE CARE\"].map({'YES': 1, 'NO': 0})\n",
    "X = data.drop(['INTUBATION', 'INTENSIVE CARE'], axis=1)\n",
    "Y = data[\"INTENSIVE CARE\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f594ee3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1362, 34)\n",
      "(1362,)\n"
     ]
    }
   ],
   "source": [
    "#Printing out our second model shapes\n",
    "print(X.shape)\n",
    "print(Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "555a69d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=0)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#We fitted our Train data for our 2nd classification\n",
    "classifier = RandomForestClassifier(n_estimators = 200, random_state = 0)\n",
    "classifier.fit(X_Train,Y_Train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e4cd4ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Predicting Test data throughout classifier variable\n",
    "Y_Pred = classifier.predict(X_Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "53c156ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9560117302052786\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Printing out the Random Forest accuracy score\n",
    "accuracy_score_rf = accuracy_score(Y_Test, Y_Pred)\n",
    "print(accuracy_score_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a77adccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Receiving algorithm performance measure formulas as below\n",
    "tn, fp, fn, tp = confusion_matrix(Y_Test, Y_Pred).ravel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d0300f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We imported Python's Numpy library and created two empty arrays\n",
    "import numpy as np\n",
    "sensitivity_rf = np.empty(0)\n",
    "specificity_rf = np.empty(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4e5e47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving Specificity and Sensitivity formulas from 4 variable\n",
    "sensitivity_rf = np.append(sensitivity_rf, np.array([tp / (tp + fn)]))\n",
    "specificity_rf = np.append(specificity_rf, np.array([tn / (tn + fp)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "85816200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "#Printing out the Specificity and Sensitivity variables\n",
    "print(sensitivity_rf)\n",
    "print(specificity_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "34df15a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We imported GradientBoostingClassifier and fitted our Train data\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gradient_boosting = GradientBoostingClassifier(n_estimators = 100, max_depth = 3)\n",
    "gradient_boosting.fit(X_Train, Y_Train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5f19544e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.9 %\n"
     ]
    }
   ],
   "source": [
    "#Printing out the Gradient Boosting accuracy score\n",
    "acc_gradient = round(gradient_boosting.score(X_Train, Y_Train) * 100, 2)\n",
    "print(round(acc_gradient,2,), \"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7880a875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9384164222873901\n"
     ]
    }
   ],
   "source": [
    "#Predicting Test data throughout classifier variable and process same operations as we did before\n",
    "prediction_gb = gradient_boosting.predict(X_Test)\n",
    "accuracy_score_gb = accuracy_score(prediction_gb, Y_Test)\n",
    "print(accuracy_score_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6cf138dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3]\n",
      "[0.97819315]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_Test, prediction_gb).ravel()\n",
    "sensitivity_gb = np.empty(0)\n",
    "specificity_gb = np.empty(0)\n",
    "sensitivity_gb = np.append(sensitivity_gb, np.array([tp / (tp + fn)]))\n",
    "specificity_gb = np.append(specificity_gb, np.array([tn / (tn + fp)]))\n",
    "print(sensitivity_gb)\n",
    "print(specificity_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32c431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b69fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c735529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
